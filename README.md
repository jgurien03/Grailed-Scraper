
# Data Visualization Through Grailed

## Background:

Recently, I've been buying and selling a lot of clothes on the website Grailed, which got me interested in the prices of brands that I'm interested in and how they've changed over the years. So, I decided to develop a program to capture, clean, and visualize this data into an easily interpretable way.

## The Process:

As this is a data collection tool, I started the program by opening a selenium driver in Chrome on Grailed's login page. Then, I created a function to login to Grailed, because in order to view sold listings, you have to be signed in. To do this, I had to implement a captcha bypass using OpenAI's Whisper program, which allowed me to transcribe and paste in the text (credits to @theriley106 for this). Then, I implemented two options: one being the brand the user would like to navigate too, and the other being if they would like to view either current or sold listings. This would allow for an easier sense for data collection, as the brand and item availability all have effects on the data, each one with their own unique numbers. Once the options were submitted, I could start developing the scroll. I used selenium to scroll from the top to the bottom of the page 'infinitely' (I say this with quotes because Grailed only lets drivers scroll for 1000 feed items). Once scrolling through all of the data, I used BeautifulSoup4 to capture each unique feature of each item, those being the titles, current prices, original prices, sizes, current listing dates, original listing dates, image links, and listing links. These were then converted to a Pandas DataFrame, from which I could then optimize and use to visualize and make conclusions about my data.

## The Problems/How I addressed Them:

As much as I would have liked to just stopped here, there was still a slew of issues present in the way that I was collecting my data. For one, Grailed has a massive issue with false positives, where users will list the brand of clothing as said brand, but it ends up being something different in the description. This is generally used to garner more views and hits in their algorithm, but it isn't very good for data collection, as it can drastically change our numbers. To fix this, I filtered out all rows in the data that didn't have the brand listed in the title. And while this may have removed some actual data, I believe that it did far better than worse, as I generally removed ~100-200 false positives per 1000 item scan. Fixing this helped, but I still had many issues to fix, such as needing to collect the categories of each clothing item for my database. Grailed's category system isn't very good, and if I tried to implement it, I wouldn't be able to generate a lot of data, so I had to categorize items after the parsing was complete. To do this, I manually created a dictionary consisting of each category: accessories, bottoms, dresses, outerwear, shoes, skirts, and tops, and their unique garments. This worked out for the most part, but what about the cases where the brand was listed, but not a category? To fix, I trained a deep neural network using TensorFlow and Keras to classify each image based on their categories. To generate this model, I first had to create an image database. My first attempt was to use the fashion_mnist's database and train a model based on that, but it didn't work, since each image was set with a black background and grayscale imaging. Even after preprocessing my data to match the dataset's it still didn't work, since none of my images were ever truly going to be able to match fashion_mnist's level of preprocessing. Next, I decided to use the dataset put out by @alexeygrigorev called 'clothing-dataset-small.' This dataset consisted of around 5000 images with 10 categories, which worked better, but not good enough. So to solve this, I generated my own dataset on top of this. I grabbed each image src via my image link array, and then downloaded each image into separate train and validation folders, each consisting of my 7 categories. I then implemented an ImageHash function to remove duplicates, which would help prevent overfitting in my data. Finally, my dataset was complete, and it was time to train my model.

## Training the First Model:

For my first model, I decided to use the pretrained 'imagenet' classifier models, since they were the best bang for my buck, given my dataset only had around ~6500 images to go off of. After many tests, I went with the Xception model, as its great object detection worked perfectly with my sometimes hard-to-read garments that were included in the dataset. From there, I implemented gradient descent via the adam optimizer, and used categorical crossentropy to work with my multiclass system. I then preproccessed the input of my train and validation images, and set their dimensions to 150 by 150, as this gave me the best results for the least amount of hassle in generating the model. I then was ready to generate the model, which I did with a learning rate of .001 to best optimize time for performance, as lower learning rates didn't yield any better results. I also trained it on 18 epochs, and used EarlyStopping after 2 epochs to prevent any overfitting. I then saved this model to my current directory, and stopped there. The train accuracy and loss was 92% and 30% respectively, and the validation accuracy and loss was 75% and 72% respectively. This high validation loss can be explained by a lack of diverse training data, which I will fix soon in the future by providing more images for it to be trained on. All problems aside, my data was complete, and it was finally time to plot it.

## Plotting the Data

To plot the data, I decided to go with Matplotlib, as its usability with Pandas is always easy and effective. To start, I chose to plot my data based on four categories: volume, price per category, price over time, and price by size. All of these showed unique features of my data which could be interpreted in their own ways. To plot by volume, I simply took the counts of each category, and plotted them on a bar graph, with the x-axis being the categories and the y-axis being the volume. For my next graph, I took all of the unique categories, and looped through each one. Then, I indexed into each unique category and took the mean of all of their original prices, which I then converted to a bar graph, with each bar on the x-axis being a different category and the y-axis representing the price. For my third graph, I decided to have multiple options, those being to graph by days, months, and years. This would give the user unique options to view the price at different intervals, so they could make accurate and knowledgeable predictions. I also provided the option to narrow the data to only show information for one category, instead of just for the entire brand. But before I could any plotting, I first had to get the actual dates of the items listed. Grailed doesn't do things by date, but rather, by intervals of time, such as minutes, hours, days, months, and years. They also provide keywords such as about, almost, over, and sold, which I promptly removed from the data. Then to convert the intervals to actual dates, I used the datetime and timedelta libraries to extract the proper dates, which would go into their own unique column titled 'Relative Date.' Then, I could finally start the plotting. First, I did the 'days' option, which started by creating a new 'Days' column. From there, I filtered the DataFrame to only include items from the last 31 days (1 month). Then, I calculated the average price and volume of each day, and then plotted it as a line graph, which I thought was appropriate since this was a day-by-day time graph. Next, I created my months graph, which was done by grouping all months together by combining their respective year and respective month. Then, I generated the volume of each months by taking the count of each month's items and normalizing it. Then, I implemented a bar graph to plot the data, and included a color gradient to represent the volume of each bar, with dark purple being the lowest and light yellow being the highest. Finally, I created the years graph by grouping all of the years into their own respective categories, and, after preprocessing, generating, and normalizing each year's volume, I plotted each year as a bar on a bar graph, complete with the same color gradient for volume. Finally, the last graph that I created was a size graph, which would take the group all of the sizes by their mean and count to generate volume, and then would take their price and print it as a bar graph in ascending order, from lowest price to highest price. I also included the same volume slider to see the most popular and least popular sizes.

## Problems With Plotting

For the most part, plotting these graphs went along great, but I did have a few issues. The first issue I came across had to do with trying to plot data that didn't exist. Grailed lists their sold listings in descending order from most recent sale to oldest sale,and in the event that all 1000 items I parsed through contained the 'days' keyword, that meant that there wouldn't be any option to plot months or years. This would cause errors when I tried to, so I implemented a catch to scan if there were only days featured in the data, and if that was true, then it would default to printing the days graph. Another issue that I came across was in my months graph, where there would accidentally be an artificially high volume generated at any month that was a year ago from the current date. This happened because once listings go over a year old, they display as being sold '1 year ago.' Effectively, this keyword removed that month-year feature that my graph was using, and would instead just group all listings under the month from a year ago. For example, when I was testing, the month June 2022, would appear with an incredibly high volume, since that month was the current month that was a year ago from when I was testing the program. To solve this, I simply checked to make sure that the date would only get fed into the graph if it was in between 11 months ago and today, which fixed my volume issue.

## Next Steps

After collecting, cleaning, and plotting my data, there was still one more thing that I wanted to do - make predictions about future data. To do this, I decided to train another model using PyTorch's LSTM module. To start, I first modified my dataset to contain month, year, volume, and price columns. Doing this would allow for my data to be properly preprocessed into the LSTM module. Once the dataset was created, I then trained my model by scalar transforming the dataset's price and volume features. Doing this normalized my data, so it could properly be interpreted. After doing this, I then created two NumPy arrays called X and Y, which would be fed my normalized data over the last 12 months. Then, I performed a train/test split by dividing up 80/20. After this, I converted the train and test sets to PyTorch tensors, making sure to reshape the y train and test tensors to (-1, 1) in order for them to properly match with my LSTM output. After this, I defined my loss function as MSE, and my optimizer as Adam (gradient descent). Then, I trained the model over 100 epochs at a .01 learning rate, to get a train/test loss as ~.00027 and ~.0003 respectively. Then once my model was trained, I could then train it to predict future data. To do this, I first normalized the last available data, and from there, extracted the current year and month. Then, I set the future data to equal the current data, and began to evaluate my model. I looped through the number of future months the user wanted to predict, and created an input tensor each time. From there, I transformed the tensor to calculate the next month's year, and then reshaped and preprocessed it to predict the next month's price. Then, I added random fluctuations to stimulate noise, as a means to mirror how trends work. Then, I created the next sequence by appending the predicted price, which would be fed into the future data. This would then become the current sequence, and this prediction process would repeat until the number of months were finished looping through. Then, I transformed and reshaped the future data, and made it so it only contained the data from the number of months the user specified. Then, I generated future months to be paired with the future prices, and converted the prices to local USD, and returned them, along with their corresponding months. 

## Problems with LSTM

Over the course of training this model, I ran into a significant problem, which I will list now for clarity. Firstly, the model can only be trained properly if there are 14 months of past data, which, for some cases, doesn't happen. So to solve this, I made it so that the dataset generated would be fed random fluctuating numbers in these empty months. The prices and volume would always be on the lower side, as to not significantly impact the actual data. By doing this, I was able to have the model generate results, though they may not be as accurate as the true data. Even still, I was able to get my test loss down to around ~.2 with this method, which, while being much worse than my true test loss, is still pretty good given the random inputs into the data. I also included a torch seed as a way to make the prices consistent, so they won't sway too much with each rerun of the program.

## Final Thoughts

Overall, this project provided a useful insight into how fashion trends change over time. With the boom of social medias such as TikTok in the fashion community, I could actively see how certain echochambers and groups on social media contributed to the rising prices of certain brands on Grailed, which was amazing to see. 

## How To Run

To run this program, simple type the command streamlit run d:/streamlit-grailed/main.py in your terminal after cloning this repository and it will launch the streamlit app. Then, you can simply follow the labels given on the webpage, and you can extract, interact with, and download your customized data. Also, please run through the entire webpage to properly reset and configure models for training/testing. If you have any further questions or concerns about the project, you can email me at jgurien@ucsc.edu

At the moment, I also have a discord bot working which can be found at this link: https://github.com/jgurien03/grailed-bot 

Owners - Jake Gurien, Nico Vitagliano
